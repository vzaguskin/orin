import numpy as np
from rknn.api import RKNN
import os
from tts_vocab import vocab
import sounddevice as sd
import torch
from torch import nn
import soundfile as sf
from scipy import signal

MAX_LENGTH = 200
MODEL_DIR = "models/mms-tts/"
ENCODER = "mms_tts_eng_encoder_200.rknn"
DECODER = "mms_tts_eng_decoder_200.rknn"

def run_encoder(encoder_model, input_ids_array, attention_mask_array):
    if 'rknn' in str(type(encoder_model)):
        log_duration, input_padding_mask, prior_means, prior_log_variances = encoder_model.inference(inputs=[input_ids_array, attention_mask_array])
    elif 'onnx' in str(type(encoder_model)):
        log_duration, input_padding_mask, prior_means, prior_log_variances = encoder_model.run(None, {"input_ids": input_ids_array, "attention_mask": attention_mask_array})

    return log_duration, input_padding_mask, prior_means, prior_log_variances

def run_decoder(decoder_model, attn, output_padding_mask, prior_means, prior_log_variances):
    if 'rknn' in str(type(decoder_model)):
        waveform  = decoder_model.inference(inputs=[attn, output_padding_mask, prior_means, prior_log_variances])[0]
    elif 'onnx' in str(type(decoder_model)):
        waveform  = decoder_model.run(None, {"attn": attn, "output_padding_mask": output_padding_mask, "prior_means": prior_means, "prior_log_variances": prior_log_variances})[0]

    return waveform

def pad_or_trim(token_id, attention_mask, max_length):
    pad_len = max_length - len(token_id)
    if pad_len <= 0:
        token_id = token_id[:max_length]
        attention_mask = attention_mask[:max_length]

    if pad_len > 0:
        token_id = token_id + [0] * pad_len
        attention_mask = attention_mask + [0] * pad_len

    return token_id, attention_mask

def preprocess_input(text, vocab, max_length):
    text = list(text.lower())
    input_id = []
    for token in text:
        if token not in vocab:
            continue
        input_id.append(0)
        input_id.append(int(vocab[token]))
    input_id.append(0)
    attention_mask = [1] * len(input_id)

    input_id, attention_mask = pad_or_trim(input_id, attention_mask, max_length)

    input_ids_array = np.array(input_id)[None,...]
    attention_mask_array = np.array(attention_mask)[None,...]

    return input_ids_array, attention_mask_array

def middle_process(log_duration, input_padding_mask, max_length):
    log_duration = torch.tensor(log_duration)
    input_padding_mask = torch.tensor(input_padding_mask)

    speaking_rate = 1
    length_scale = 1.0 / speaking_rate
    duration = torch.ceil(torch.exp(log_duration) * input_padding_mask * length_scale)
    predicted_lengths = torch.clamp_min(torch.sum(duration, [1, 2]), 1).long()

    # Create a padding mask for the output lengths of shape (batch, 1, max_output_length)
    predicted_lengths_max_real = predicted_lengths.max()
    predicted_lengths_max = max_length * 2

    indices = torch.arange(predicted_lengths_max, dtype=predicted_lengths.dtype)
    output_padding_mask = indices.unsqueeze(0) < predicted_lengths.unsqueeze(1)
    output_padding_mask = output_padding_mask.unsqueeze(1).to(input_padding_mask.dtype)

    # Reconstruct an attention tensor of shape (batch, 1, out_length, in_length)
    attn_mask = torch.unsqueeze(input_padding_mask, 2) * torch.unsqueeze(output_padding_mask, -1)
    batch_size, _, output_length, input_length = attn_mask.shape
    cum_duration = torch.cumsum(duration, -1).view(batch_size * input_length, 1)
    indices = torch.arange(output_length, dtype=duration.dtype)
    valid_indices = indices.unsqueeze(0) < cum_duration
    valid_indices = valid_indices.to(attn_mask.dtype).view(batch_size, input_length, output_length)
    padded_indices = valid_indices - nn.functional.pad(valid_indices, [0, 0, 1, 0, 0, 0])[:, :-1]
    attn = padded_indices.unsqueeze(1).transpose(2, 3) * attn_mask

    attn = attn.numpy()
    output_padding_mask = output_padding_mask.numpy()
    
    return attn, output_padding_mask, predicted_lengths_max_real

def play_audio_resample(waveform, samplerate=16000):
    # –¶–µ–ª–µ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ ‚Äî 48000 –ì—Ü (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è RT5616)
    target_sr = 48000

    # –ï—Å–ª–∏ —á–∞—Å—Ç–æ—Ç–∞ —É–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è ‚Äî –ø—Ä–æ—Å—Ç–æ –∏–≥—Ä–∞–µ–º
    if samplerate == target_sr:
        sd.play(waveform, samplerate=target_sr, device=0)
        sd.wait()
        return

    # –ò–Ω–∞—á–µ ‚Äî —Ä–µ—Å—ç–º–ø–ª–∏—Ä—É–µ–º
    print(f"üîÑ –†–µ—Å—ç–º–ø–ª–∏–Ω–≥ —Å {samplerate} –ì—Ü ‚Üí {target_sr} –ì—Ü...")

    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–µ—Å—ç–º–ø–ª–∏–Ω–≥–∞
    ratio = target_sr / samplerate
    num_samples = int(len(waveform) * ratio)

    # –†–µ—Å—ç–º–ø–ª–∏–Ω–≥ —Å –ø–æ–º–æ—â—å—é scipy
    waveform_resampled = signal.resample(waveform, num_samples)

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1]
    waveform_resampled = np.clip(waveform_resampled, -1.0, 1.0)

    # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º
    sd.play(waveform_resampled, samplerate=target_sr, device=2)
    sd.wait()

import time

def play_audio(waveform, samplerate=16000):
    waveform = np.asarray(waveform, dtype=np.float32)
    if np.max(np.abs(waveform)) > 1.0:
        waveform = waveform / np.max(np.abs(waveform))

    duration_sec = len(waveform) / samplerate  # ‚Üê –í–´–ß–ò–°–õ–Ø–ï–ú –î–õ–ò–¢–ï–õ–¨–ù–û–°–¢–¨
    print(f"üîä –ü—Ä–æ–∏–≥—Ä—ã–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ... (–¥–ª–∏–Ω–∞: {len(waveform)} —Å—ç–º–ø–ª–æ–≤, ~{duration_sec:.3f}—Å)")

    sd.play(waveform, samplerate=samplerate, device=2)
    sd.wait()  # ‚Üê –ñ–¥—ë–º, –ø–æ–∫–∞ –±—É—Ñ–µ—Ä –æ—Å–≤–æ–±–æ–¥–∏—Ç—Å—è (—ç—Ç–æ –≤—Å—ë –µ—â—ë –Ω—É–∂–Ω–æ)
    
    # ‚úÖ –ê –¢–ï–ü–ï–†–¨ ‚Äî –ñ–î–Å–ú –†–ï–ê–õ–¨–ù–û–ï –í–†–ï–ú–Ø –ü–†–û–ò–ì–†–´–í–ê–ù–ò–Ø
    #time.sleep(duration_sec + 0.1)  # +0.1—Å ‚Äî –Ω–∞ –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å –¥—Ä–∞–π–≤–µ—Ä–∞
    print(f"‚úÖ –ó–≤—É–∫ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–æ–∏–≥—Ä–∞–Ω ({duration_sec:.3f}—Å)")

class TTSVocaliser:
    def __init__(self):
        encoder_path = os.path.join(MODEL_DIR, ENCODER)
        decoder_path = os.path.join(MODEL_DIR, DECODER)
        self.encoder = self.init_rknn_model(encoder_path)
        self.decoder = self.init_rknn_model(decoder_path)

    def init_rknn_model(self, model_path):
        model = RKNN()
        ret = model.load_rknn(model_path)
        ret = model.init_runtime(target="rk3588", device_id=None)
        return model
    
    def synthesize(self, inp: str):
        input_ids_array, attention_mask_array = preprocess_input(inp, vocab, max_length=MAX_LENGTH)

        # Encode
        log_duration, input_padding_mask, prior_means, prior_log_variances = run_encoder(self.encoder, input_ids_array, attention_mask_array)

        # Middle process
        attn, output_padding_mask, predicted_lengths_max_real = middle_process(log_duration, input_padding_mask, MAX_LENGTH)

        # Decode
        waveform = run_decoder(self.decoder, attn, output_padding_mask, prior_means, prior_log_variances)
        #audio_save_path = "test_output.wav"
        #sf.write(file=audio_save_path, data=np.array(waveform[0][:predicted_lengths_max_real * 256]), samplerate=16000)
    
        audio_data=np.array(waveform[0][:predicted_lengths_max_real * 256])
        return audio_data
        
    def vocalise(self, inp: str):
        audio_data = self.synthesize(inp)
        play_audio(audio_data)


if __name__ == "__main__":
    voc = TTSVocaliser()
    voc.vocalise("–ü—Ä–∏–≤–µ—Ç, —è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –û—Ä–∏–Ω, —è –≥–æ–≤–æ—Ä—é –ø–æ —Ä—É—Å—Å–∫–∏")



